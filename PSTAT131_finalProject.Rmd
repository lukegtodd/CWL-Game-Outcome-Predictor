---
title: "PSTAT 131 Final Project"
author: "Luke Todd"
date: "4/4/2022"
output: 
  html_document:
   code_folding: hide
---

```{r setup, include=FALSE, message = FALSE}

knitr::opts_chunk$set(echo = TRUE)

library(vembedr)

```

# Introduction

The purpose of this project is to take a player's indiviual statistics and attempt to determine whether they won or lost that game. I will then use this predictive capability and try to guess the winner of the largest tournament of the year, Call of Duty Champs.

--- 

# What is the Call of Duty World League?

Call of Duty is a first-person shooter that first began in 2003. Since then, it has become one of the largest multiplayer video game franchises to exist. During this time, a competitive scene for the game has gained traction. In 2016, the Call of Duty World League was born -- a sponsored league that hosts major tournaments throughout the year for the best players in the world to play in. In these events, these pros play three different game modes to decide the winner of a series. These game modes are Hardpoint, Search and Destroy, and then a third game mode that often changes yearly. For the data that we are covering, the third game mode is Control. All of the teams in the league consist of 5 players, and the series are Best of 5's.

```{r}
embed_url("https://www.youtube.com/watch?v=VQC0aZuGBFs&t=2740s")
```

---  

# Game Modes in Circuit

### Hardpoint
In Hardpoint, the two teams must fight over a point on the map where every second they spend in this point, they gain one point. This point is called the "hardpoint." If two teams are in the hardpoint at the same time, then neither teams collects points. Every sixty seconds, the hardpoint changes locations on the map, so teams must make tactical decisions to be able to rotate across the map. The first team to 250 points wins the map.

```{r}
embed_url("https://www.youtube.com/watch?v=VQC0aZuGBFs&t=2740s") %>%
  use_start_time(6*60 + 35)
```

### Search and Destroy
In Search and Destroy, the two teams play rounds where each player only has one life; if you die, you are dead until the next round. The objective is to either kill the entire other team before the time limit, or if you are on offense, then you can plant the bomb. If the bomb detonates after 45 seconds, then you also win the round. The first team to win 6 rounds wins the map.

```{r}
embed_url("https://www.youtube.com/watch?v=VQC0aZuGBFs&t=2740s") %>%
  use_start_time(18*60 + 39)
```

### Control 
In Control, there is an offense team and a defense team. There are multiple rounds where each team switches off between offense and defense. Each team has 30 lives per round. The first time to win three rounds wins the map. The offensive team is trying to either capture two points on the map, or eliminate all 30 lives of the other team. The defensive team is trying to either defend the two points before the time rounds out, or eliminate all 30 lives of the other team. 

```{r}
embed_url("https://www.youtube.com/watch?v=VQC0aZuGBFs&t=2740s") %>%
  use_start_time(45*60 + 40)
```

---  

# Why is this model useful?

This model is useful because it will allow us to see whether a player's statistics may have contributed to a win or not. As a fan of COD Competitive, there is a lot of debate on statistics and it's importance, so I wanted to look directly at the impact of a player's statistics.

---

# Load Packages

All the packages are loaded below.

```{r, include=FALSE, message = FALSE}

library(readr)
library(tidyverse)
library(tidymodels)
library(janitor)
library(sqldf)
library(sjmisc)
library(vembedr)
library(rpart)
library(rpart.plot)
library(parsnip)
library(vip)
library(randomForest)
library(dplyr)
library(tidyr)
library(data.table)
library(randomForest)
library(datasets)
library(caret)
library(ROCR)
library(tune)
library(kknn)

```

---  

# Load Data

This project makes use of official CWL data that is uploaded on Github. All data is organized relatively cleanly and all missing data is reported. 

```{r, error = FALSE, message = FALSE}

proleague2019 <- read_csv(url("https://raw.githubusercontent.com/Activision/cwl-data/master/data/data-2019-07-05-proleague.csv"))
fortworth2019 <- read_csv(url("https://raw.githubusercontent.com/Activision/cwl-data/master/data/data-2019-03-17-fortworth.csv"))
london2019 <- read_csv(url("https://raw.githubusercontent.com/Activision/cwl-data/master/data/data-2019-05-05-london.csv"))
anaheim2019 <- read_csv(url("https://raw.githubusercontent.com/Activision/cwl-data/master/data/data-2019-06-16-anaheim.csv"))
proleagueFinals2019 <- read_csv(url("https://raw.githubusercontent.com/Activision/cwl-data/master/data/data-2019-07-21-proleague-finals.csv"))

# all stats for all major tournaments (EXCEPT CHAMPS) in BO4 (2019)
majors2019 <- rbind(proleague2019, fortworth2019, london2019, anaheim2019, proleagueFinals2019)

# champs will act as our test data; we will try and predict the winner
champs2019 <- read_csv(url("https://raw.githubusercontent.com/Activision/cwl-data/master/data/data-2019-08-18-champs.csv"))

```

---  

# What data are we concerned with?

In order to determine a win for a game, we will need to address Hardpoint, Search and Destroy, and the Control separately. Each of these gamemodes have different parameters, so we will have to fit models for each gamemode.

**Hardpoint:**  
match_id — helpful for getting rid of missing data  
win — '1' for a win and '0' for a loss  
team — player's corresponding team
player — what player does the data correspond to  
mode — game mode  
k_d — kill/death ratio; used to show overall impact on the map  
assists — in addition to k/d, assists show overall support on the map
role — a role is determined for each player depending on their most common gun throughout the year 
damage_dealt — total damage done in the map  
player_spm — score per minute  
x_2 — number of two-pieces (two kills in quick succession)  
x_3 — number of three-pieces (three kills in quick succession)  
x_4 — number of four-pieces (four kills in quick succession)  

hill_time_s — hill time measured in seconds  
hill_captures — shows activity on the map  
hill_defends — shows activity on the map

**Search and Destroy:**
match_id — helpful for getting rid of missing data  
win — '1' for a win and '0' for a loss  
team — player's corresponding team
player — what player does the data correspond to  
mode — game mode  
k_d — kill/death ratio; used to show overall impact on the map  
assists — in addition to k/d, assists show overall support on the map
role — a role is determined for each player depending on their most common gun throughout the year 
damage_dealt — total damage done in the map  
player_spm — score per minute  
x_2 — number of two-pieces (two kills in quick succession)  
x_3 — number of three-pieces (three kills in quick succession)  
x_4 — number of four-pieces (four kills in quick succession)  

fb_round_ratio -- 'snd_firstbloods'/'snd_rounds' 
bomb_sneak_defuses -- sneak defuses are often in pivotal rounds  
bomb_plants -- good indicator of role  
bomb_defuses -- good indicator of role  


**Control:**
match_id — helpful for getting rid of missing data  
win — '1' for a win and '0' for a loss  
team — player's corresponding team
player — what player does the data correspond to  
mode — game mode  
k_d — kill/death ratio; used to show overall impact on the map  
assists — in addition to k/d, assists show overall support on the map
role — a role is determined for each player depending on their most common gun throughout the year 
damage_dealt — total damage done in the map  
player_spm — score per minute  
x_2 — number of two-pieces (two kills in quick succession)  
x_3 — number of three-pieces (three kills in quick succession)  
x_4 — number of four-pieces (four kills in quick succession)  

ctrl_firstbloods — first kill in a round of control
ctrl_firstdeaths — first death in a round of control
ctrl_captures — number of captures in a control game

---

# Data Split


---  

# Data Cleaning and Organization

The data below is for all of the majors throughout the season, except for COD Champs. We will reserve COD Champs to act as a test set. The raw data from each major is merged into one major dataset, further broken up into Hardpoint, SND, and Control datasets.  

## All Majors 2019 data
```{r}

# CLEANING
majors2019 <- majors2019 %>% clean_names(.)

# new dataset that contains all of the missing data, just in case
majors2019_missing <- sqldf('SELECT * FROM majors2019 WHERE match_id LIKE "missing%"')

# whole event data, all players and all maps, where player names are organized alphabetically
majors2019 <- majors2019[order(majors2019$player),]

# removes missing values
majors2019 <- sqldf('SELECT * FROM majors2019 WHERE match_id NOT LIKE "missing%"')

# calculates all the players that have played more than 50 games
player_numgames <- count(majors2019, player) %>% subset(., n > 50) %>% remove_cols(n)

# includes all existing data for all players that have played more than 50 games (arbitrary number)
majors2019 <- sqldf('SELECT * FROM majors2019 WHERE player IN player_numgames')

# removes all matches where damage = 0; almost always occurs as a result of data loss
majors2019 <- sqldf('SELECT * FROM majors2019 WHERE damage_dealt != "0"')

# changes W to 1, L to 0
majors2019$win <- ifelse(majors2019$win == "W", 1, 0) %>%
  as.factor()

# assigning a role to each player to allow for more precise comparisons
playerRoles <- majors2019 %>%
  group_by(player) %>%
  count(player, fave_weapon) %>%
  top_n(1, n) %>%
  mutate(role = fave_weapon) %>%
  subset(select = -c(fave_weapon, n))

# replace fav gun with corresponding role
playerRoles$role <- str_replace(playerRoles$role, "Saug 9mm", "1")
playerRoles$role <- str_replace(playerRoles$role, "Maddox RFB", "2")
playerRoles$role <- str_replace(playerRoles$role, "ICR-7", "3")

# making factors
playerRoles$role <- factor(playerRoles$role)

# manually adjustment for player TJHaly
playerRoles <- playerRoles[-c(83), ]

majors2019 <- dplyr::inner_join(playerRoles, majors2019, by = "player")

```

A player's role is defined as a sub (1), flex (2), or an ar (3).

### Hardpoint subset
```{r}

# all 2019 hardpoint data
hp2019 <- sqldf('SELECT player, k_d, role, win, kills, deaths, x, assists, damage_dealt, player_spm, hill_time_s, hill_captures, hill_defends, x2_piece, x3_piece, x4_piece FROM majors2019 WHERE mode == "Hardpoint"')
hp2019 <- hp2019[order(hp2019$player),]

```

### Search and Destroy subset
```{r}

# all 2019 SND data
snd2019 <- sqldf('SELECT match_id, team, player, role, win, kills, deaths, k_d, assists, damage_dealt, player_spm, bomb_sneak_defuses, bomb_plants, bomb_defuses, snd_rounds, snd_firstbloods, snd_1_kill_round, snd_2_kill_round, snd_3_kill_round, snd_4_kill_round, x2_piece, x3_piece, x4_piece FROM majors2019 WHERE mode == "Search & Destroy"')

# adds new column with fb/round ratio
snd2019 <- add_column(snd2019, fb_round_ratio = snd2019$snd_firstbloods/snd2019$snd_rounds)

# adding a new column with average first bloods for the season
snd2019 <- snd2019 %>%
  group_by(player) %>%
  mutate(fb_avg = mean(snd_firstbloods))

# puts data in alphabetical order
snd2019 <- snd2019[order(snd2019$player),]

```

### Control subset
```{r}

# all 2019 CONTROL data
control2019 <- sqldf('SELECT player, role, win, k_d, assists, damage_dealt, player_spm, x2_piece, x3_piece, x4_piece, ctrl_firstbloods, ctrl_firstdeaths, ctrl_captures FROM majors2019 WHERE mode == "Control"')
control2019 <- control2019[order(control2019$player),]

```

## Champs 2019 dataset
```{r}

champs2019 <- champs2019 %>% clean_names(.)
champs2019 <- champs2019[order(champs2019$player),]
champs2019 <- sqldf('SELECT * FROM champs2019 WHERE match_id NOT LIKE "missing%"')
champs2019 <- sqldf('SELECT * FROM champs2019 WHERE damage_dealt != "0"')

# changes W to 1, L to 0
champs2019$win <- ifelse(champs2019$win == "W", 1, 0) %>%
  as.factor()

champs2019 <- dplyr::inner_join(playerRoles, champs2019, by = "player")

```

### Hardpoint CHAMPS subset
```{r}

# CHAMPS 2019 hardpoint data
hpChamps <- sqldf('SELECT team, end_time, match_id, player, k_d, role, win, kills, deaths, x, assists, damage_dealt, player_spm, hill_time_s, hill_captures, hill_defends, x2_piece, x3_piece, x4_piece FROM champs2019 WHERE series_id = "champs-bracket-grand-finals-0" AND mode = "Hardpoint"')
hpChamps <- hpChamps[order(hpChamps$team),]

```

### Search and Destroy CHAMPS subset
```{r}

# CHAMPS 2019 SND data
sndChamps <- sqldf('SELECT team, end_time, match_id, player, win, role, k_d, assists, damage_dealt, player_spm, bomb_sneak_defuses, bomb_plants, bomb_defuses, snd_rounds, snd_firstbloods, x2_piece, x3_piece, x4_piece, snd_1_kill_round, snd_2_kill_round, snd_3_kill_round, snd_4_kill_round FROM champs2019 WHERE mode == "Search & Destroy"')

# adds new column with fb/round ratio
sndChamps <- add_column(sndChamps, fb_round_ratio = sndChamps$snd_firstbloods/sndChamps$snd_rounds)

# adding a new column with average first bloods for the season
sndChamps <- sndChamps %>%
  group_by(player) %>%
  mutate(fb_avg = mean(snd_firstbloods))

# puts data in alphabetical order
sndChamps <- sndChamps[order(sndChamps$team),]

```

### Control CHAMPS subset
```{r}

# CHAMPS 2019 CONTROL data
controlChamps <- sqldf('SELECT team, end_time, ctrl_firstbloods, ctrl_firstdeaths, ctrl_captures, x2_piece, x3_piece, x4_piece, player, role, win, k_d, assists, damage_dealt, player_spm FROM champs2019 WHERE mode == "Control"')
controlChamps <- controlChamps[order(controlChamps$team),]

```






----------------------------------------------------------  





# Exploratory Data Analysis
For my exploratory data analysis, I will be using just the season data. It will not include the Champs data.



## Kill/death for season
```{r, fig.height = 25, fig.width = 9}

ggplot(majors2019, aes(x = reorder(player, k_d), y = k_d)) + geom_boxplot() + coord_flip(ylim = c(0, 3.5)) + labs(y = "Kill/death ratio", x = "Player", subtitle = "OVERALL Player K/D's, 2019 Season (BO4), Descending")
ggplot(hp2019, aes(x = reorder(player, k_d), y = k_d)) + geom_boxplot() + coord_flip(ylim = c(0, 3.5)) + labs(y = "Kill/death ratio", x = "Player", subtitle = "Player K/D's for HARDPOINT, 2019 Season (BO4), Descending")
ggplot(snd2019, aes(x = reorder(player, k_d), y = k_d)) + geom_boxplot() + coord_flip(ylim = c(0, 5)) + labs(y = "Kill/death ratio", x = "Player", subtitle = "Player K/D's for SEARCH AND DESTROY, 2019 Season (BO4), Descending")
ggplot(control2019, aes(x = reorder(player, k_d), y = k_d)) + geom_boxplot() + coord_flip(ylim = c(0, 3.5)) + labs(y = "Kill/death ratio", x = "Player", subtitle = "Player K/D's for CONTROL, 2019 Season (BO4), Descending")

```



## Search and Destroy First Bloods
Search and Destroy is a gamemode that has multiple rounds, where in each round, every player only has one life. A "first blood" is the first kill of the round and is usually highly influential. This a common stat that commentators and the community look at.

### Firstblood average
```{r, fig.height = 25, fig.width = 11}

# player firstblood average for SND 2019

ggplot(snd2019, aes(x = reorder(player, fb_avg), y = fb_avg)) + geom_point() + coord_flip(ylim = c(0, 3)) + labs(y = "Firstblood Average", x = "Player", subtitle = "Player Firstblood Average for SEARCH AND DESTROY, 2019 Season (BO4), Descending")

```


### Firstblood totals
```{r, fig.height = 25, fig.width = 9}

# player firstbloods for SND 2019

ggplot(snd2019, aes(x = reorder(player, snd_firstbloods), y = snd_firstbloods)) + geom_boxplot() + coord_flip(ylim = c(0, 6)) + labs(y = "Firstbloods", x = "Player", subtitle = "Player Firstbloods for SEARCH AND DESTROY, 2019 Season (BO4), Descending")

```


### Firstblood/round
```{r, fig.height = 25, fig.width = 11}

# player firstblood/round for SND 2019

ggplot(snd2019, aes(x = reorder(player, fb_round_ratio), y = fb_round_ratio)) + geom_boxplot() + coord_flip(ylim = c(0, 0.6)) + labs(y = "Firstblood/round ratio", x = "Player", subtitle = "Player Firstblood/Round for SEARCH AND DESTROY, 2019 Season (BO4), Descending")

```



## Overall Damage Dealt
```{r, fig.height = 25, fig.width = 11}

# player damage dealt OVERALL 2019

# removes all entries where damage is 0; this is almost always a result of data loss
majors2019 <- sqldf('SELECT * FROM majors2019 WHERE damage_dealt != "0"')
playerDamage <- sqldf('SELECT player, damage_dealt FROM majors2019 WHERE damage_dealt != "0"')

ggplot(playerDamage, aes(x = reorder(player, damage_dealt), y = damage_dealt)) + geom_boxplot() + coord_flip(ylim = c(0, 10000)) + labs(y = "Damage Dealt", x = "Player", subtitle = "OVERALL Player Damage Dealt, 2019 Season (BO4), Descending")

```



## Overall Score/Min (spm)
```{r, fig.height = 25, fig.width = 11}

# Overall score per minute for 2019 season

ggplot(majors2019, aes(x = reorder(player, player_spm), y = player_spm)) + geom_boxplot() + coord_flip(ylim = c(0, 675)) + labs(y = "Score per minute", x = "Player", subtitle = "OVERALL Player Score per minute, 2019 Season (BO4), Descending")

```



## Number of Wins
```{r, fig.height = 25, fig.width = 11}

# Overall number of wins for 2019 season

playerwins <- sqldf('SELECT player, win FROM majors2019 WHERE win == "1"') # selects all the wins for each player
playerwins <- playerwins %>% count(player) # counts the number of wins per player

ggplot(playerwins, aes(x = reorder(player, n), y = n)) + geom_bar(stat = 'identity') + coord_flip() + labs(y = "Number of Wins", x = "Player", subtitle = "OVERALL Number of Wins per Player, 2019 Season (BO4), Descending")

```

The top 4 players with the most amount of wins in the season are Slasher, Octane, Kenny, and Enable. The interesting part about this is that all of these players were on the same team, 100 Thieves. They all tied with 116 wins during the season. 


```{r}

playerwins %>%
  ggplot(aes(x = n)) + geom_histogram(binwidth = 15, color = "black", fill = "white")

```

The number of wins appears to follow a normal distribution. The left side of the histogram appears to be slightly more populated, but I hypothesize that this is due to players that didn't play for the whole season.





-----------------------------------------------------------------------





# Models: Hardpoint
I will be trying to predict whether an individual player will win or lose a game based on his statistics in the given game.

## Splitting Data:
Here, I am splitting the Hardpoint data with 80% training and 20% testing. The data is stratified on the "win" variable.

```{r}

hp2019_wl <- hp2019

set.seed(3068)

hp2019_wlsplit <- hp2019_wl %>%
  initial_split(prop = 0.8, strata = "win")

hp2019_train <- training(hp2019_wlsplit)
hp2019_test <- testing(hp2019_wlsplit)

```

Below is the head of the training data; as well as the dimensions for the training and the testing data. There is also the distribution for the number of wins.
```{r}

head(hp2019_train)

dim(hp2019_train)
dim(hp2019_test)

prop.table(table(hp2019_train$win))

```

---

## Creating a recipe and folding:
To begin with my Hardpoint models, I made a recipe that contained all of my predictor variables. I then normalized all of my predictor variables.

```{r}

hp_recipe <- recipe(win ~ k_d + assists + damage_dealt + 
                        player_spm + hill_time_s + hill_captures + 
                        hill_defends + x2_piece + x3_piece + x4_piece, 
                      data = hp2019_train) %>%
  step_normalize(all_predictors())

```

After making my recipe, I decided to fold my data with 10 folds and 5 repeats. 
```{r}

hp_train_folds <- vfold_cv(hp2019_train, v = 10, repeats = 5)

```

--------------

## Model 1: Decision Tree

Creating a general decision tree specification using rpart:
```{r}

hp_tree_spec <- decision_tree() %>%
  set_engine("rpart")

```

Setting a classification decision tree engine:
```{r}

hp_class_tree_spec <- hp_tree_spec %>%
  set_mode("classification")

```

Fitting the model:
```{r}

hp_class_tree_fit <- hp_class_tree_spec %>%
  fit(win ~ k_d + assists + damage_dealt + 
                        player_spm + hill_time_s + hill_captures + 
                        hill_defends + x2_piece + x3_piece + x4_piece, 
                      data = hp2019_train)

```

Visualizing the decision tree:
```{r, warning = FALSE}

hp_class_tree_fit %>%
  extract_fit_engine() %>%
  rpart.plot(roundint=FALSE)

```

Checking confusion matrix and accuracy of the **train** data:
```{r}

augment(hp_class_tree_fit, new_data = hp2019_train) %>%
  conf_mat(truth = win, estimate = .pred_class)

hp_dt_accuracy <- augment(hp_class_tree_fit, new_data = hp2019_train) %>%
  accuracy(truth = win, estimate = .pred_class)
hp_dt_accuracy

```

Creating a workflow that is ready to tune cost complexity:
```{r}

hp_class_tree_wf <- workflow() %>%
  add_model(hp_class_tree_spec %>% set_args(cost_complexity = tune())) %>%
  add_recipe(hp_recipe)

```

Setting up a regular grid:
```{r}

param_grid <- grid_regular(cost_complexity(range = c(-3, -1)), levels = 10)

```

Fitting and tuning our model:
```{r}

hp_dt_tune <- hp_class_tree_wf %>%
  tune_grid(
  hp_class_tree_wf, 
  resamples = hp_train_folds, 
  grid = param_grid)

```

Plotting our model, which shows what cost-complexity produces the highest accuracy:
```{r}

autoplot(hp_dt_tune)

```

Selecting the best performing value and finalizing the workflow:
```{r}

hp_best_complexity <- select_best(hp_dt_tune)

hp_class_tree_final <- finalize_workflow(hp_class_tree_wf, hp_best_complexity)

hp_class_tree_final_fit <- fit(hp_class_tree_final, data = hp2019_train)

```

Visualizing the final model:
```{r}

hp_class_tree_final_fit %>%
  extract_fit_engine() %>%
  rpart.plot(roundint = FALSE)

```

Checking the accuracy of the final model:
```{r}

hp_tuned_dt_accuracy <- augment(hp_class_tree_final_fit, new_data = hp2019_train) %>%
  accuracy(truth = win, estimate = .pred_class)
hp_tuned_dt_accuracy

```

As we can see, the final accuracy for the tuned model was slightly higher with an estimate of 0.7321881, compared to that of the untuned model with an estimate of 0.7203605.




--------------------------------------------



## Model 2: Random Forest

Now it was time to prepare my model. I tuned min_n and mtry, set my mode to "classification", and set my engine to "ranger." My workflow was set up to use both my Hardpoint recipe and my Hardpoint random forest model.
```{r}

hp_rf_model <- rand_forest(min_n = tune(),
                        mtry = tune(),
                        mode = "classification") %>%
  set_engine("ranger")

hp_rf_workflow <- workflow() %>%
  add_model(hp_rf_model) %>%
  add_recipe(hp_recipe)

```

Next, I set up parameters for the grid that I was going to make. The parameters were set the Hardpoint random forest model and the mtry range was set from 1 to 7. This mtry limit was set slightly below the maximum number of predictors.
```{r}

hp_rf_parameters <- hardhat::extract_parameter_set_dials(hp_rf_model) %>%
  update(mtry = mtry(range = c(2, 10)))

hp_rf_grid <- grid_regular(hp_rf_parameters, levels = 2)

```

Then, I ran my model by tuning and fitting, using my folded data and my grid. 
```{r}

hp_rf_tune <- hp_rf_workflow %>%
  tune_grid(resamples = hp_train_folds,
            grid = hp_rf_grid)

```

The last thing to do was to plot my tuned model.
```{r}

autoplot(hp_rf_tune)

```

As we can see from the above plot, it appears that as we add more predictor variables, the accuracy tends to decrease. I hypothesize that this is because k_d is the most significant predictor by far. All of the other predictors are much less significant, and actually worsen the model by overfitting. However, the decrease in accuracy is very small in general.  

Checking the accuracy of the final model:
```{r}

hp_rf_tuned_accuracy <- show_best(hp_rf_tune, metric = "accuracy")
hp_rf_tuned_accuracy
hp_rf_tuned_accuracy[1,5]

```

We had the highest accuracy of 0.744 with a minimum node size of 2 and an mtry of 2.

----------


## Model 3: Logistic Regression

First, I needed to set up my model. I set my engine to "glm" for logistic regression and set the mode to "classification."
```{r}
hp_log_reg <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")
```

Set up workflow with the model I created last step, as well as the recipe that I created earlier.
```{r}
hp_log_wkflow <- workflow() %>% 
  add_model(hp_log_reg) %>% 
  add_recipe(hp_recipe)
```

Fit the model to the folded data:
```{r}
hp_log_fit <- fit_resamples(hp_log_wkflow, hp_train_folds)
```

Collecting metrics based on the folded data:
```{r}
collect_metrics(hp_log_fit)
```

Fitting the model to the whole dataset, not just the folds:
```{r}
hp_log_fit_train <- fit(hp_log_wkflow, hp2019_train)
```

Assessing model performance with the training data:
```{r}

predict(hp_log_fit_train, new_data = hp2019_train, type = "prob")

augment(hp_log_fit_train, new_data = hp2019_train) %>%
  conf_mat(truth = win, estimate = .pred_class)

augment(hp_log_fit_train, new_data = hp2019_train) %>%
  conf_mat(truth = win, estimate = .pred_class) %>%
  autoplot(type = "heatmap")

```

Checking accuracy with the training data:
```{r}

hp_log_reg_accuracy <- augment(hp_log_fit_train, new_data = hp2019_train) %>%
  accuracy(truth = win, estimate = .pred_class)
hp_log_reg_accuracy

```
The accuracy with the training data was about 0.749, which is roughly the same as the accuracy with the folded data.


-----------


## Model 4: K Nearest Neighbors

Setting up the model. I will be tuning "neighbors."
```{r}
hp_knn_model <- 
  nearest_neighbor(
    neighbors = tune(),
    mode = "classification") %>% 
  set_engine("kknn")
```

Next, I set up the workflow.
```{r}
hp_knn_workflow <- workflow() %>% 
  add_model(hp_knn_model) %>% 
  add_recipe(hp_recipe)
```

I then set up the tuning grid.
```{r}
hp_knn_parameters <- hardhat::extract_parameter_set_dials(hp_knn_model)
hp_knn_grid <- grid_regular(hp_knn_parameters, levels = 2)
```

Fitting and tuning my model:
```{r}
hp_knn_tune <- hp_knn_workflow %>%
  tune_grid(resamples = hp_train_folds,
            grid = hp_knn_grid)
```

Plotting the model:
```{r}
autoplot(hp_knn_tune, metric = "accuracy")
```

This plot shows us that as the number of neighbors increases, there is also an increase in accuracy.

Testing the accuracy of the model:
```{r}
hp_knn_tuned_accuracy <- show_best(hp_knn_tune, metric = "accuracy")
hp_knn_tuned_accuracy
```
As we can see, the model performs at its best with 15 neighbors, resulting in an accuracy of 0.7094363.


----------

## Comparing Model Performance

```{r}

hp_accuracies <- c(hp_tuned_dt_accuracy$.estimate,
                    hp_rf_tuned_accuracy[1,5],
                    hp_log_reg_accuracy$.estimate, 
                    hp_knn_tuned_accuracy[1, 4])
hp_accuracies

```

Our most accurate model was the decision tree with an accuracy of 0.755, followed by logistic regression's 0.750, random forest's 0.744, and k-th nearest neighbor's 0.710.























----------------------------------------------------------------------------


# Models: Search and Destroy
I will be trying to predict whether an individual player will win or lose a game based on his statistics in the given game.

## Splitting Data:
Here, I am splitting the Search and Destroy data with 80% training and 20% testing. The data is stratified on the "win" variable.

```{r}

set.seed(1)

snd2019_split <- snd2019 %>%
  initial_split(prop = 0.8, strata = "win")

snd2019_train <- training(snd2019_split)
snd2019_test <- testing(snd2019_split)

```

Below is the head of the training data; as well as the dimensions for the training and the testing data. There is also the distribution for the number of wins.
```{r}

head(snd2019_train)

dim(snd2019_train)
dim(snd2019_test)

prop.table(table(snd2019_train$win))

```

---

## Creating a recipe and folding:
To begin with my Search and Destroy models, I made a recipe that contained all of my predictor variables. I then normalized all of my predictor variables.

```{r}

snd_recipe <- recipe(win ~ k_d + assists + damage_dealt + 
                        player_spm + bomb_sneak_defuses + 
                        bomb_plants + bomb_defuses + snd_firstbloods + 
                       fb_round_ratio + snd_1_kill_round +
                       snd_2_kill_round + snd_3_kill_round +
                       snd_4_kill_round + x2_piece + x3_piece + x4_piece, 
                      data = snd2019_train) %>%
  step_normalize(all_predictors())

```

After making my recipe, I decided to fold my data with 10 folds and 5 repeats. 
```{r}

snd_train_folds <- vfold_cv(snd2019_train, v = 10, repeats = 5)

```

--------------

## Model 1: Decision Tree

Creating a general decision tree specification using rpart:
```{r}

snd_tree_spec <- decision_tree() %>%
  set_engine("rpart")

```

Setting a classification decision tree engine:
```{r}

snd_class_tree_spec <- snd_tree_spec %>%
  set_mode("classification")

```

Fitting the model:
```{r}

snd_class_tree_fit <- snd_class_tree_spec %>%
  fit(win ~ k_d + assists + damage_dealt + 
                        player_spm + bomb_sneak_defuses + 
                        bomb_plants + bomb_defuses + snd_firstbloods + 
                       fb_round_ratio + snd_1_kill_round +
                       snd_2_kill_round + snd_3_kill_round +
                       snd_4_kill_round + x2_piece + x3_piece + x4_piece, 
                      data = snd2019_train)

```

Visualizing the decision tree:
```{r}

snd_class_tree_fit %>%
  extract_fit_engine() %>%
  rpart.plot(roundint=FALSE)

```

Checking confusion matrix and accuracy of the **train** data:
```{r}

augment(snd_class_tree_fit, new_data = snd2019_train) %>%
  conf_mat(truth = win, estimate = .pred_class)

snd_dt_accuracy <- augment(snd_class_tree_fit, new_data = snd2019_train) %>%
  accuracy(truth = win, estimate = .pred_class)
snd_dt_accuracy

```

Creating a workflow that is ready to tune cost complexity:
```{r}

snd_class_tree_wf <- workflow() %>%
  add_model(snd_class_tree_spec %>% set_args(cost_complexity = tune())) %>%
  add_recipe(snd_recipe)

```

Setting up a regular grid:
```{r}

parameter_grid <- grid_regular(cost_complexity(range = c(-3, -1)), levels = 10)

```

Fitting and tuning our model:
```{r}

snd_rf_tune <- snd_class_tree_wf %>%
  tune_grid(resamples = snd_train_folds, grid = parameter_grid)

```

Plotting our model, which shows what cost-complexity produces the highest accuracy:
```{r}

autoplot(snd_rf_tune)

```

Selecting the best performing value and finalizing the workflow:
```{r}

snd_best_complexity <- select_best(snd_rf_tune, metric = "accuracy")

snd_class_tree_final <- finalize_workflow(snd_class_tree_wf, snd_best_complexity)

snd_class_tree_final_fit <- fit(snd_class_tree_final, data = snd2019_train)

```

Visualizing the final model:
```{r}

snd_class_tree_final_fit %>%
  extract_fit_engine() %>%
  rpart.plot(roundint = FALSE)

```

Checking the accuracy of the final model:
```{r}

snd_tuned_dt_accuracy <- augment(snd_class_tree_final_fit, new_data = snd2019_train) %>% accuracy(truth = win, estimate = .pred_class)
snd_tuned_dt_accuracy

```

As we can see, the final accuracy for the tuned model was slightly higher with an estimate of 0.7416697, compared to that of the untuned model with an estimate of 0.7001075.




--------------------------------------------



## Model 2: Random Forest

Now it was time to prepare my model. I tuned min_n and mtry, set my mode to "classification", and set my engine to "ranger." My workflow was set up to use both my SND recipe and my SND random forest model.
```{r}

snd_rf_model <- rand_forest(min_n = tune(),
                        mtry = tune(),
                        mode = "classification") %>%
  set_engine("ranger")

snd_rf_workflow <- workflow() %>%
  add_model(snd_rf_model) %>%
  add_recipe(snd_recipe)

```

Next, I set up parameters for the grid that I was going to make. The parameters were set with the SND random forest model and the mtry range was set from 1 to 10. This mtry limit was set slightly below the maximum number of predictors.
```{r}

snd_rf_parameters <- hardhat::extract_parameter_set_dials(snd_rf_model) %>%
  update(mtry = mtry(range = c(1, 10)))

snd_rf_grid <- grid_regular(snd_rf_parameters, levels = 2)

```

Then, I ran my model by tuning and fitting, using my folded data and my grid. 
```{r}

snd_rf_tune <- snd_rf_workflow %>%
  tune_grid(resamples = snd_train_folds,
            grid = snd_rf_grid)

```

The last thing to do was to plot my tuned model.
```{r}

autoplot(snd_rf_tune)

```

As we can see from the above plot, when we increase the number of predictors, our accuracy and ROC AUC slightly increases. I hypothesize that this is different from the Hardpoint plot because there are more predictors that are significant in Search and Destroy.

Checking the accuracy of the final model:
```{r}

snd_rf_tuned_accuracy <- show_best(snd_rf_tune, metric = "accuracy")
snd_rf_tuned_accuracy
snd_rf_tuned_accuracy[1,5]

```

We had the highest accuracy of 0.7091295 with a minimum node size of 40 and an mtry of 10.

----------


## Model 3: Logistic Regression

First, I needed to set up my model. I set my engine to "glm" for logistic regression and set the mode to "classification."
```{r}
snd_log_reg <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")
```

Setting up workflow with the model I created last step, as well as the recipe that I created earlier.
```{r}
snd_log_wkflow <- workflow() %>% 
  add_model(snd_log_reg) %>% 
  add_recipe(snd_recipe)
```

Fit the model to the folded data:
```{r, warning = FALSE}
snd_log_fit <- fit_resamples(snd_log_wkflow, snd_train_folds)
```

Collecting metrics based on the folded data:
```{r}
collect_metrics(snd_log_fit)
```

Fitting the model to the whole dataset, not just the folds:
```{r, warning=FALSE}
snd_log_fit_train <- fit(snd_log_wkflow, snd2019_train)
```

Assessing model performance with the training data:
```{r}

predict(snd_log_fit_train, new_data = snd2019_train, type = "prob")

augment(snd_log_fit_train, new_data = snd2019_train) %>%
  conf_mat(truth = win, estimate = .pred_class)

augment(snd_log_fit_train, new_data = snd2019_train) %>%
  conf_mat(truth = win, estimate = .pred_class) %>%
  autoplot(type = "heatmap")

```

Checking accuracy with the training data:
```{r}

snd_log_reg_accuracy <- augment(snd_log_fit_train, new_data = snd2019_train) %>%
  accuracy(truth = win, estimate = .pred_class)
snd_log_reg_accuracy

```



-----------


## Model 4: K Nearest Neighbors

Setting up the model. I will be tuning "neighbors."
```{r}
snd_knn_model <- 
  nearest_neighbor(
    neighbors = tune(),
    mode = "classification") %>% 
  set_engine("kknn")
```

Next, I set up the workflow.
```{r}
snd_knn_workflow <- workflow() %>% 
  add_model(snd_knn_model) %>% 
  add_recipe(snd_recipe)
```

I then set up the tuning grid.
```{r}
snd_knn_parameters <- hardhat::extract_parameter_set_dials(snd_knn_model)
snd_knn_grid <- grid_regular(snd_knn_parameters, levels = 2)
```

Fitting and tuning my model:
```{r}
snd_knn_tune <- snd_knn_workflow %>%
  tune_grid(resamples = snd_train_folds,
            grid = snd_knn_grid)
```

Plotting the model:
```{r}
autoplot(snd_knn_tune, metric = "accuracy")
```
This plot shows us that as the number of neighbors increases, there is also an increase in accuracy.

Testing the accuracy of the model:
```{r}
snd_knn_tuned_accuracy <- show_best(snd_knn_tune, metric = "accuracy")
snd_knn_tuned_accuracy
```
As we can see, the model performs at its best with 15 neighbors, resulting in an accuracy of 0.6188415.


----------

## Comparing Model Performance

```{r}

snd_accuracies <- c(snd_tuned_dt_accuracy$.estimate,
                    snd_rf_tuned_accuracy[1,5],
                    snd_log_reg_accuracy$.estimate, 
                    snd_knn_tuned_accuracy[1, 4])
snd_accuracies

```

As we can see from this, the decision tree appears to have the highest accuracy.


----------------------------------------------------------------------------


# Models: Control
I will be trying to predict whether an individual player will win or lose a game based on his statistics in the given game.

## Splitting Data:
Here, I am splitting the Search and Destroy data with 80% training and 20% testing. The data is stratified on the "win" variable.

```{r}

set.seed(1)

control2019_split <- control2019 %>%
  initial_split(prop = 0.8, strata = "win")

control2019_train <- training(control2019_split)
control2019_test <- testing(control2019_split)

```

Below is the head of the training data; as well as the dimensions for the training and the testing data. There is also the distribution for the number of wins.
```{r}

head(control2019_train)

dim(control2019_train)
dim(control2019_test)

prop.table(table(control2019_train$win))

```

---

## Creating a recipe and folding:
The begin with my Control models, I made a recipe that contained all of my predictor variables. I then normalized all of my predictor variables.

```{r}

control_recipe <- recipe(win ~ k_d + assists + damage_dealt + 
                        player_spm + ctrl_firstbloods +
                          ctrl_firstdeaths + ctrl_captures + 
                          x2_piece + x3_piece + x4_piece, 
                      data = control2019_train) %>%
  step_normalize(all_predictors())

```

After making my recipe, I decided to fold my data with 10 folds and 5 repeats. 
```{r}

control_train_folds <- vfold_cv(control2019_train, v = 10, repeats = 5)

```

--------------

## Model 1: Decision Tree

Creating a general decision tree specification using rpart:
```{r}

control_tree_spec <- decision_tree() %>%
  set_engine("rpart")

```

Setting a classification decision tree engine:
```{r}

control_class_tree_spec <- control_tree_spec %>%
  set_mode("classification")

```

Fitting the model:
```{r}

control_class_tree_fit <- control_class_tree_spec %>%
  fit(win ~ k_d + assists + damage_dealt + 
                        player_spm + ctrl_firstbloods +
                          ctrl_firstdeaths + ctrl_captures + 
                          x2_piece + x3_piece + x4_piece, 
                      data = control2019_train)

```

Visualizing the decision tree:
```{r}

control_class_tree_fit %>%
  extract_fit_engine() %>%
  rpart.plot()

```

Checking confusion matrix and accuracy of the **train** data:
```{r}

augment(control_class_tree_fit, new_data = control2019_train) %>%
  conf_mat(truth = win, estimate = .pred_class)

control_dt_accuracy <- augment(control_class_tree_fit, new_data = control2019_train) %>%
  accuracy(truth = win, estimate = .pred_class)
control_dt_accuracy

```

Creating a workflow that is ready to tune cost complexity:
```{r}

control_class_tree_wf <- workflow() %>%
  add_model(control_class_tree_spec %>% set_args(cost_complexity = tune())) %>%
  add_recipe(control_recipe)

```

Setting up a regular grid:
```{r}

parameter_grid <- grid_regular(cost_complexity(range = c(-3, -1)), levels = 10)

```

Fitting and tuning our model:
```{r}

control_rf_tune <- control_class_tree_wf %>%
  tune_grid(resamples = control_train_folds, grid = parameter_grid)

```

Plotting our model, which shows what cost-complexity produces the highest accuracy:
```{r}

autoplot(control_rf_tune)

```

Selecting the best performing value and finalizing the workflow:
```{r}

control_best_complexity <- select_best(control_rf_tune, metric = "accuracy")

control_class_tree_final <- finalize_workflow(control_class_tree_wf, control_best_complexity)

control_class_tree_final_fit <- fit(control_class_tree_final, data = control2019_train)

```

Visualizing the final model:
```{r}

control_class_tree_final_fit %>%
  extract_fit_engine() %>%
  rpart.plot(roundint = FALSE)

```

Checking the accuracy of the final model:
```{r}

control_tuned_dt_accuracy <- augment(control_class_tree_final_fit, new_data = control2019_train) %>% accuracy(truth = win, estimate = .pred_class)
control_tuned_dt_accuracy

```

As we can see, the final accuracy for the tuned model was slightly higher with an estimate of 0.827, compared to that of the untuned model with an estimate of 0.732.




--------------------------------------------



## Model 2: Random Forest

Now it was time to prepare my model. I tuned min_n and mtry, set my mode to "classification", and set my engine to "ranger." My workflow was set up to use both my Hardpoint recipe and my Hardpoint random forest model.
```{r}

control_rf_model <- rand_forest(min_n = tune(),
                        mtry = tune(),
                        mode = "classification") %>%
  set_engine("ranger")

control_rf_workflow <- workflow() %>%
  add_model(control_rf_model) %>%
  add_recipe(control_recipe)

```

Next, I set up parameters for the grid that I was going to make. The parameters were set the Hardpoint random forest model and the mtry range was set from 1 to 9. This mtry limit was set slightly below the maximum number of predictors.
```{r}

control_rf_parameters <- hardhat::extract_parameter_set_dials(control_rf_model) %>%
  update(mtry = mtry(range = c(1, 9)))

control_rf_grid <- grid_regular(control_rf_parameters, levels = 2)

```

Then, I ran my model by tuning and fitting, using my folded data and my grid. 
```{r}

control_rf_tune <- control_rf_workflow %>%
  tune_grid(resamples = control_train_folds,
            grid = control_rf_grid)

```

The last thing to do was to plot my tuned model.
```{r}

autoplot(control_rf_tune)

```

As we can see from the above plot, when we increase the number of predictors, our accuracy and ROC and AUC slightly increases. I hypothesize that this is different from the Hardpoint plot because there are more predictors that are significant in Control.

Checking the accuracy of the final model:
```{r}

control_rf_tuned_accuracy <- show_best(control_rf_tune, metric = "accuracy")
control_rf_tuned_accuracy
control_rf_tuned_accuracy[1,5]

```

We had the highest accuracy of 0.749 with a minimum node size of 40 and an mtry of 9.

----------


## Model 3: Logistic Regression

First, I needed to set up my model. I set my engine to "glm" for logistic regression and set the mode to "classification."
```{r}
control_log_reg <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")
```

Setting up workflow with the model I created last step, as well as the recipe that I created earlier.
```{r}
control_log_wkflow <- workflow() %>% 
  add_model(control_log_reg) %>% 
  add_recipe(control_recipe)
```

Fit the model to the folded data:
```{r, warning = FALSE}
control_log_fit <- fit_resamples(control_log_wkflow, control_train_folds)
```

Collecting metrics based on the folded data:
```{r}
collect_metrics(control_log_fit)
```

Fitting the model to the whole dataset, not just the folds:
```{r, warning=FALSE}
control_log_fit_train <- fit(control_log_wkflow, control2019_train)
```

Assessing model performance with the training data:
```{r}

predict(control_log_fit_train, new_data = control2019_train, type = "prob")

augment(control_log_fit_train, new_data = control2019_train) %>%
  conf_mat(truth = win, estimate = .pred_class)

augment(control_log_fit_train, new_data = control2019_train) %>%
  conf_mat(truth = win, estimate = .pred_class) %>%
  autoplot(type = "heatmap")

```

Checking accuracy with the training data:
```{r}

control_log_reg_accuracy <- augment(control_log_fit_train, new_data = control2019_train) %>%
  accuracy(truth = win, estimate = .pred_class)
control_log_reg_accuracy

```

The final model had an accuracy of the 


-----------


## Model 4: K Nearest Neighbors

Setting up the model. I will be tuning "neighbors."
```{r}
control_knn_model <- 
  nearest_neighbor(
    neighbors = tune(),
    mode = "classification") %>% 
  set_engine("kknn")
```

Next, I set up the workflow.
```{r}
control_knn_workflow <- workflow() %>% 
  add_model(control_knn_model) %>% 
  add_recipe(control_recipe)
```

I then set up the tuning grid.
```{r}
control_knn_parameters <- hardhat::extract_parameter_set_dials(control_knn_model)
control_knn_grid <- grid_regular(control_knn_parameters, levels = 2)
```

Fitting and tuning my model:
```{r}
control_knn_tune <- control_knn_workflow %>%
  tune_grid(resamples = control_train_folds,
            grid = control_knn_grid)
```

Plotting the model:
```{r}
autoplot(control_knn_tune, metric = "accuracy")
```
This plot shows us that as the number of neighbors increases, there is also an increase in accuracy.

Testing the accuracy of the model:
```{r}
control_knn_tuned_accuracy <- show_best(control_knn_tune, metric = "accuracy")
control_knn_tuned_accuracy
```
As we can see, the model performs at its best with 15 neighbors, resulting in an accuracy of 0.6188415.


----------

## Comparing Model Performance

```{r}

control_accuracies <- c(control_tuned_dt_accuracy$.estimate,
                    control_rf_tuned_accuracy[1,5],
                    control_log_reg_accuracy$.estimate, 
                    control_knn_tuned_accuracy[1, 4])
control_accuracies

```

As we can see from this, the decision tree appears to have the highest accuracy.






---------------

# Model Finalization

According to my models, the decision tree appeared to be the best model for all three of our game modes. To move forward, we will now finalize our model for each game mode and fit our test data.

## Hardpoint
I will begin fitting the final models with Hardpoint.
```{r}
hp_metric <- metric_set(accuracy)

hp_model_test <- predict(hp_class_tree_final_fit, new_data = hp2019_test)  %>% 
  bind_cols(hp2019_test %>% select(win)) 
hp_model_test

hp_model_test %>%
  hp_metric(truth = win, estimate = .pred_class)
```

The final accuracy for Hardpoint test data was 0.693. This is less than what we got on our train data, but is roughly the same.

## Search and Destroy
Next, I will fit the Search and Destroy test data.
```{r}
snd_metric <- metric_set(accuracy)

snd_model_test <- predict(snd_class_tree_final_fit, new_data = snd2019_test)  %>% 
  bind_cols(snd2019_test %>% select(win)) 
snd_model_test

snd_model_test %>%
  snd_metric(truth = win, estimate = .pred_class)
```

The final accuracy for Search and Destroy test data was 0.697. This is less than what we got on our train data, but is roughly the same.

## Control
Next, I will fit the Search and Destroy test data.
```{r}
control_metric <- metric_set(accuracy)

control_model_test <- predict(control_class_tree_final_fit, new_data = control2019_test)  %>% 
  bind_cols(control2019_test %>% select(win)) 
control_model_test

control_model_test %>%
  control_metric(truth = win, estimate = .pred_class)
```

The final accuracy for Control test data was 0.729. This is much less than what we got on our train data, where the accuracy was 0.827.

## Discussion
I expect that most of my models had lower accuracy for my test data compared to the train data because of overfitting. I had a feeling that this would happen since the final decision trees were much more complex than the ones before they were tuned. Regardless, I am happy with an accuracy of 0.70 for each model. For a game that is largely a team effort, being able to predict a win with 0.70 by only looking at one player's statistics is not too bad.

# Attempting to Predict the COD Champs Final
The COD Champs tournament is the largest tournament of the year. During Black Ops 4, the two teams that competed in the grand finals were eUnited and 100 Thieves. I will be using my models to try and predict the winner of this grand finals.

## Map 1: Hardpoint
First, I will predict the first Hardpoint map. I created a dataframe which only contained the data from the first map.

```{r}

map1_hp <- sqldf('SELECT * FROM hpChamps where end_time = "2019-08-18 21:19:01 UTC"')

map1_hp_prediction <- predict(hp_class_tree_final_fit, map1_hp) %>%
  cbind(map1_hp$player, map1_hp$team)
map1_hp_prediction

```

My model predicted that Enable, Kenny, and Priestahh would lose, whereas Octane and Slasher would win. On the other hand, every player on eUnited was predicted to win by my model. 

Even though Octane's and Slasher's statistics were individually adequate for the win, because of the rest of their team's performance and because of the predictions for eUnited, **I would predict that eUnited won the first Hardpoint map.**

In reality, eUnited did win the first map 250-126! So far, so good.

**My prediction: 1-0 eUnited**
**Reality: 1-0 eUnited**

## Map 2: Search and Destroy
Next, I will look at Map 2, Search and Destroy.

```{r}

map2_snd <- sqldf('SELECT * FROM sndChamps where end_time = "2019-08-18 21:37:20 UTC"')

map2_snd_prediction <- predict(snd_class_tree_final_fit, map2_snd) %>%
  cbind(map2_snd$player, map2_snd$team)
map2_snd_prediction

```
My model predicted that Priestahh would win from 100 Thieves, and that Abezy and Simp would win from eUnited. Based on the majority, I would predict that eUnited would win the second map.

In reality, 100 Thieves won the second map 6-3.

**My prediction: 2-0 eUnited**
**Reality: 1-1 Tied**

## Map 3: Control
Next, I will look at Map 3, Control.

```{r}

map3_control <- sqldf('SELECT * FROM controlChamps where end_time = "2019-08-18 21:55:55 UTC"')

map3_control_prediction <- predict(control_class_tree_final_fit, map3_control) %>%
  cbind(map3_control$player, map3_control$team)
map3_control_prediction

```

My model predicted that Kenny, Octane, and Priestahh would win from 100 Thieves, and only Abezy would win from eUnited. Thus, I would predict that 100 Thieves would win this match.

In reality, 100 Thieves did win the third map, 3-1! Another success, phew.

**My prediction: 2-1 eUnited**
**Reality: 2-1 100 Thieves**

## Map 4: Hardpoint 
Next, I will look at Map 4, Hardpoint.

```{r}

map4_hp <- sqldf('SELECT * FROM hpChamps where end_time = "2019-08-18 22:11:21 UTC"')

map4_hp_prediction <- predict(hp_class_tree_final_fit, map4_hp) %>%
  cbind(map4_hp$player, map4_hp$team)
map4_hp_prediction

```

My model predicted that Kenny, Octane, and Priestahh would win from 100 Thieves, and that Abezy, Arcitys, and Simp would win from eUnited. This is an apparent flaw in my approach of determining an overall winner, as this in theory should be a tie. For the sake of reaching map 5, the final map that decides the tournament, let's just say that 100 Thieves wins.

In reality, eUnited wins map 4 250-219, but again, we will predict otherwise just for the sake of moving forward.

**My prediction: 2-2 Tie**
**Reality: 2-2 Tie**

## Map 5: Search and Destroy
This is what is all comes down to! My model has gotten us to this final map! Albeit, through a different path. Lastly, I will predict the winner of the final Search and Destroy to determine the winner of the whole event.

```{r}

map5_snd <- sqldf('SELECT * FROM sndChamps where end_time = "2019-08-18 22:32:13 UTC"')

map5_snd_prediction <- predict(snd_class_tree_final_fit, map5_snd) %>%
  cbind(map5_snd$player, map5_snd$team)
map5_snd_prediction

```

My model predicted that nobody on 100 Thieves would win, and that Abezy, Arcitys, Clayster, and Prestinni would win from eUnited. Thus, eUnited is my overall predicted winner!

In reality, eUnited did win the final map 6-4! My model was correct in the end!

If you would like to see the actual series between 100 Thieves and eUnited, I have linked it below.

```{r}
embed_url("https://www.youtube.com/watch?v=AfWqhymZnoc")
```


# Conclusion

## Discussion
I expect that most of my models had lower accuracy for my test data compared to the train data because of overfitting. I had a feeling that this would happen since the final decision trees were much more complex than the ones before they were tuned. Regardless, I am happy with an accuracy of 0.70 for each model. For a game that is largely a team effort, being able to predict a win with 0.70 by only looking at one player's statistics is not too bad.

## Player Statistic Take Aways
Apart from all the research and code that is currently shown in this project, there were also hundreds of lines of code that didn't make the cut. I tried multiple different methods of modeling, multiple different ways of organizing data, and multiple different metrics that weren't included in the original data set. I tried to approach it from both an individual player's perspective, as well as a team's perspective. Ultimately, as the project discusses, I went with an individual player's statistics. From this research, I have learned a lot about the predictive variables that determine a win or a loss for a player.

One thing that I immediately noticed was the massive importance of Kill/Death (KD) ratio. In practically every model, KD was the most important metric in determining a win or a loss. It overpowered most other metrics, which was a shock for me. For example, as discussed before, First Bloods in Search and Destroy are thought to be very important to a win. However, statistically, it didn't even appear in some of the models.

Another thing that I noticed was how unimportant other metrics were. Some metrics that I thought would be important, weren't. However, I would be interested in seeing how some of these metrics might be weighted differently if you approach it from a team perspective.

## Future Research
In the future, I believe that a more effective approach for predicting a win or a loss would be by looking at a team's statistics as a whole. You can take this even further by ranking each team using their past statistics and using these to predict future matches; an unsupervised learning experiment. 

Additionally, the data that I used for this research was from the CWL in 2018. Since then, the CWL has transformed into the CDL (Call of Duty League), with more focus being placed on data. There are more predictors to look at that just weren't available for the CWL data. Future research should use CDL data as there are some interesting parameters, such as untraded kills, that might provide some more predictive power. I also think it would be interesting to look at interactions between teams and maps, and try and use this to aid in predicting a winner.

## Final Thoughts
This project was a great opportunity to get hands on with the R language and machine learning concepts. Being able to work with Call of Duty data gave me a reason to keep working, and gave me an intrinsic pre-understanding of the data. There is still a lot to learn when it comes to machine learning, but as soon as I learn it, I hope to apply it to my future research ideas. All in all, it gave me a better understanding of a player's statistics in Call of Duty and will give me an extra thing to think about whenever I watch future competitive matches.














